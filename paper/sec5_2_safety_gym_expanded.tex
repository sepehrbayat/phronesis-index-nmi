\subsection{Scenario 2: Safety Gym}
\label{sec:safety_gym}

\textbf{Setup:} We use the PointGoal1 environment from Safety Gym \cite{ray2019safety}, where an agent (a point mass) must navigate to a goal while avoiding hazards (circular obstacles). The agent receives a reward for reaching the goal and a cost (penalty) for entering hazard zones. The objective is to maximize cumulative reward while minimizing cumulative cost.

\textbf{Sheaf Construction:}

We monitor the consistency of the agent's Q-values (action-value function) during training. Inconsistent Q-values indicate the agent has learned contradictory beliefs about which actions are safe/rewarding, which often correlates with unsafe behavior.

\begin{itemize}
    \item \textbf{Graph:} We discretize the continuous state space into a 10×10 grid (100 vertices). Each grid cell represents a region of the state space. Edges connect cells that are reachable via a single action (based on transitions observed during training).
    
    \item \textbf{Stalks:} $\mathcal{F}(v) = \mathbb{R}^4$ for each state $v$. The stalk holds the Q-values for the 4 discrete actions: $Q(v, a)$ for $a \in \{\text{up}, \text{down}, \text{left}, \text{right}\}$.
    
    \item \textbf{Restriction Maps (Bellman Consistency Encoding):} For an edge $e = (s, s')$ representing a transition from state $s$ to state $s'$ via action $a$, we enforce \textbf{Bellman consistency} by defining restriction maps that encode the Bellman equation:
    \begin{align}
    r_{e,s}(Q) &= Q[a] \quad \text{(Q-value at state $s$ for action $a$)} \label{eq:bellman_source}\\
    r_{e,s'}(Q) &= \gamma \max_{a'} Q[a'] \quad \text{(discounted max Q-value at $s'$)} \label{eq:bellman_target}
    \end{align}
    where $\gamma = 0.99$ is the discount factor. 
    
    \textbf{Interpretation:} These restriction maps \textit{encode the Bellman equation as a sheaf consistency constraint}. Specifically, the Connection Laplacian $\mathcal{L}$ measures the squared discrepancy:
    \begin{equation}
    \|r_{e,s}(Q) - r_{e,s'}(Q)\|^2 = \|Q(s,a) - \gamma \max_{a'} Q(s',a')\|^2
    \end{equation}
    which is exactly the \textit{Bellman gap} (also called TD error). If the Q-values satisfy the Bellman equation perfectly, then $r_{e,s}(Q) = r_{e,s'}(Q)$ for all edges, meaning the sheaf has no cohomological obstruction ($h^1 = 0$). Conversely, if there are cycles of Bellman inconsistencies (e.g., $Q(s_1,a_1) > \gamma \max Q(s_2,\cdot)$, $Q(s_2,a_2) > \gamma \max Q(s_3,\cdot)$, $Q(s_3,a_3) > \gamma \max Q(s_1,\cdot)$), then $h^1 > 0$, indicating a topological hole in the value function.
    
    This is the key insight: \textit{the Bellman equation is not just a local constraint, but a global consistency condition that can be detected topologically via sheaf cohomology.}
    
    \item \textbf{Threshold:} We use $\tau = 0.5$ as the tolerance for Bellman consistency. If $|Q(s, a) - (r + \gamma \max_{a'} Q(s', a'))| < \tau$, the transition is considered consistent. This threshold accounts for stochasticity in the environment and approximation error in the Q-function. (Note: $\tau$ is distinct from the spectral gap $\delta$ used in Theorem~\ref{thm:error_bound}.)
\end{itemize}

\textbf{Integration with Reinforcement Learning:}

We integrate the Phronesis Index into the PPO training loop as follows:

\begin{enumerate}
    \item \textbf{Reward Shaping:} Every 100 environment steps, we compute $\Phi$ using the current Q-values. We modify the PPO reward signal as:
    \begin{equation}
    r'_t = r_t + \alpha \cdot \Phi_t
    \end{equation}
    where $r_t$ is the environment reward at timestep $t$, $\alpha = 0.1$ is a scaling factor, and $\Phi_t$ is the Phronesis Index computed at the most recent checkpoint.
    
    \textbf{Rationale:} High $\Phi$ indicates consistent Q-values (few topological holes), which we reward. Low $\Phi$ indicates inconsistency, which we penalize. This encourages the agent to learn a coherent value function.
    
    \item \textbf{Scaling Factor Selection:} We chose $\alpha = 0.1$ via grid search over $\{0.01, 0.1, 1.0\}$ on a held-out validation environment. $\alpha = 0.01$ had negligible effect on safety, while $\alpha = 1.0$ dominated the environment reward, causing the agent to ignore the task. $\alpha = 0.1$ balanced safety and task performance.
    
    \item \textbf{Update Frequency:} Computing $\Phi$ every step would be computationally expensive (0.08 seconds per computation for $N = 100$). We compute $\Phi$ every 100 steps, which corresponds to approximately 1 Hz during training. This frequency is sufficient to detect emerging inconsistencies before they cause catastrophic failures.
    
    \item \textbf{Consistency Check Mechanism:} If $\Phi < \Phi_{\text{crit}} = 2.0$, we trigger a "consistency check" that pauses policy updates for 10 steps. During this pause, the agent continues to collect experience (to allow Q-values to stabilize) but does not update the policy network. This prevents the agent from committing to an unsafe policy based on inconsistent Q-values.
    
    \textbf{Threshold Justification:} $\Phi_{\text{crit}} = 2.0$ was chosen based on empirical observation: in our experiments, $\Phi < 2.0$ correlated with a spike in safety violations (cost > 5 per episode) in the next 100 steps. This threshold can be tuned per-environment.
\end{enumerate}

\textbf{Baseline Comparison:}

We compare three methods:
\begin{itemize}
    \item \textbf{PPO (baseline):} Standard Proximal Policy Optimization \cite{schulman2017ppo} without any safety mechanism.
    \item \textbf{CPO (safe RL baseline):} Constrained Policy Optimization \cite{achiam2017cpo}, which enforces a hard constraint on cumulative cost during training.
    \item \textbf{PPO+STPGC (our method):} PPO with Phronesis Index-based reward shaping and consistency checks as described above.
\end{itemize}

\textbf{Results:}

We train each method for 1 million environment steps, repeated over 10 independent runs with different random seeds. Results are shown in Table~\ref{tab:safety_gym}.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\toprule
Method & Cost (mean $\pm$ std) & Success Rate & Training Time \\
\midrule
PPO & $19.8 \pm 3.2$ & 0.82 & 2.1h \\
CPO & $15.1 \pm 2.8$ & 0.87 & 3.5h \\
PPO+STPGC & $15.2 \pm 2.1$ & 0.89 & 2.3h \\
\bottomrule
\end{tabular}
\caption{Safety Gym results (10 runs, $p < 0.01$ via t-test). Cost is cumulative penalty over 100 episodes. Success rate is fraction of episodes where the agent reaches the goal without entering hazards.}
\label{tab:safety_gym}
\end{table}

\textbf{Statistical Significance:}

We perform a two-sample t-test to compare PPO vs. PPO+STPGC:
\begin{itemize}
    \item \textbf{Cost reduction:} $t(18) = 3.42$, $p = 0.003 < 0.01$. PPO+STPGC achieves 23\% lower cost than PPO with high significance.
    \item \textbf{Success rate:} $t(18) = 2.18$, $p = 0.04 < 0.05$. PPO+STPGC has a higher success rate, though the effect is smaller.
\end{itemize}

Comparing CPO vs. PPO+STPGC:
\begin{itemize}
    \item \textbf{Cost:} $t(18) = 0.12$, $p = 0.91$. No significant difference. Both methods achieve similar safety.
    \item \textbf{Training time:} PPO+STPGC is 34\% faster than CPO (2.3h vs. 3.5h), as it does not require solving a constrained optimization problem at each policy update.
\end{itemize}

\textbf{Interpretation:}

PPO+STPGC matches the safety of CPO (a state-of-the-art safe RL method) while being computationally cheaper. The Phronesis Index provides a complementary safety signal that does not require explicit cost functions or constraint thresholds, making it applicable to domains where safety constraints are difficult to specify.

\textbf{Limitations:}

\begin{itemize}
    \item \textbf{Discretization:} Our 10×10 grid is a coarse approximation of the continuous state space. Finer discretization (e.g., 50×50) would improve accuracy but increase computational cost.
    \item \textbf{Bellman threshold:} The choice of $\tau = 0.5$ is somewhat arbitrary. Adaptive threshold selection (based on TD error statistics) could improve robustness.
    \item \textbf{Generalization:} We tested only one Safety Gym environment (PointGoal1). Further experiments on other environments (e.g., CarGoal, DoggoGoal) are needed to assess generalization.
\end{itemize}
