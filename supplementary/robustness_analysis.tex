% Robustness Analysis and Noise Handling
% Addressing Reviewer 2's Concerns about Practical Deployment

\section{Robustness Analysis}
\label{sec:robustness}

Real-world multi-agent systems operate in noisy, uncertain environments. This section analyzes the robustness of the Phronesis Index to various sources of noise and provides practical strategies for maintaining accuracy under adverse conditions.

%--------------------------------------------------

\subsection{Sources of Noise in Multi-Agent Systems}

\subsubsection{Sensor Noise}

\textbf{Description:} Agents' observations are corrupted by measurement errors.

\textbf{Effect on Sheaf:} Stalks contain noisy beliefs $\tilde{x}_v = x_v + \eta_v$ where $\eta_v \sim \mathcal{N}(0, \sigma^2 I)$.

\textbf{Propagation:} Noise propagates through restriction maps, affecting the Connection Laplacian:
\begin{equation}
\tilde{\mathcal{L}} = \mathcal{L} + E
\end{equation}
where $E$ is a perturbation matrix with $\|E\| \leq c \sigma$ for some constant $c$ depending on graph structure.

\subsubsection{Communication Errors}

\textbf{Description:} Information shared between agents is corrupted during transmission.

\textbf{Effect on Sheaf:} Restriction maps are perturbed: $\tilde{r}_{e,v} = r_{e,v} + \Delta r_{e,v}$.

\textbf{Modeling:} We model this as additive noise in the Laplacian blocks corresponding to edges.

\subsubsection{Discretization Error}

\textbf{Description:} Continuous state spaces are discretized into finite graphs, introducing approximation errors.

\textbf{Effect on Sheaf:} The graph $G$ is an approximation of the true continuous manifold. Eigenvalues of $\mathcal{L}$ approximate those of a continuous Laplace-Beltrami operator, with error $O(h^2)$ where $h$ is the discretization step size.

\subsubsection{Model Uncertainty}

\textbf{Description:} Restriction maps are based on approximate models (e.g., linearized dynamics, simplified physics).

\textbf{Effect on Sheaf:} The sheaf structure itself is an approximation of the true consistency relationships.

%--------------------------------------------------

\subsection{Sensitivity Analysis}

\subsubsection{Eigenvalue Perturbation Theory}

\textbf{Weyl's Theorem (Restated):}

For symmetric matrices $A$ and $B = A + E$ with eigenvalues $\lambda_i(A)$ and $\lambda_i(B)$:
\begin{equation}
|\lambda_i(B) - \lambda_i(A)| \leq \|E\|_2
\end{equation}

\textbf{Application to $\Phi$:}

Let $\mathcal{L}_0$ be the ideal Laplacian and $\mathcal{L} = \mathcal{L}_0 + E$ be the noisy version with $\|E\| \leq \sigma$.

\paragraph{Effect on $\lambda_1$:}
\begin{equation}
|\lambda_1(\mathcal{L}) - \lambda_1(\mathcal{L}_0)| \leq \sigma
\end{equation}

\textbf{Relative Error:}
\begin{equation}
\frac{|\lambda_1(\mathcal{L}) - \lambda_1(\mathcal{L}_0)|}{\lambda_1(\mathcal{L}_0)} \leq \frac{\sigma}{\lambda_1(\mathcal{L}_0)}
\end{equation}

\textbf{Interpretation:} If $\lambda_1$ is large (strong consensus dynamics), the relative error is small. If $\lambda_1$ is small (weak coupling), noise has a larger relative impact.

\paragraph{Effect on $h^1$:}

From Theorem 2, the error in counting near-zero eigenvalues is:
\begin{equation}
|h^1_{\epsilon}(\mathcal{L}) - h^1_{\epsilon}(\mathcal{L}_0)| \leq \left\lceil \frac{2\sigma}{\delta} \right\rceil
\end{equation}
where $\delta$ is the spectral gap.

\textbf{Interpretation:} A large spectral gap $\delta$ makes $h^1$ estimation robust to noise. A small gap makes it sensitive.

\paragraph{Combined Effect on $\Phi$:}

\begin{equation}
\Phi = \frac{\lambda_1}{h^1 + \epsilon}
\end{equation}

\textbf{Error Propagation:}

Using first-order Taylor expansion:
\begin{equation}
\Delta \Phi \approx \frac{\partial \Phi}{\partial \lambda_1} \Delta \lambda_1 + \frac{\partial \Phi}{\partial h^1} \Delta h^1
\end{equation}

\begin{equation}
= \frac{1}{h^1 + \epsilon} \Delta \lambda_1 - \frac{\lambda_1}{(h^1 + \epsilon)^2} \Delta h^1
\end{equation}

\textbf{Worst-Case Bound:}

\begin{equation}
|\Delta \Phi| \leq \frac{\sigma}{h^1 + \epsilon} + \frac{\lambda_1}{(h^1 + \epsilon)^2} \cdot \frac{2\sigma}{\delta}
\end{equation}

\textbf{Simplified:}

\begin{equation}
|\Delta \Phi| \leq \sigma \left( \frac{1}{h^1 + \epsilon} + \frac{2\lambda_1}{\delta(h^1 + \epsilon)^2} \right)
\end{equation}

\textbf{Key Insight:} The error in $\Phi$ is linear in noise level $\sigma$ and inversely proportional to spectral gap $\delta$. Systems with good spectral properties are naturally robust.

\subsubsection{Illustrative Sensitivity Example}

To illustrate the theoretical bounds, consider the Logic Maze scenario (5$\times$5 grid, 25 vertices) with spectral gap $\delta \approx 0.5$ and $\lambda_1 \approx 1.5$:
\begin{itemize}
    \item For noise $\sigma = 0.01$: the bound gives $|\Delta \lambda_1| \leq 0.01$, so $\Phi$ changes by $< 1\%$. The condition $\sigma < \delta/4 = 0.125$ is easily satisfied.
    \item For noise $\sigma = 0.1$: the bound gives $|\Delta \lambda_1| \leq 0.1$, and $\lceil 2\sigma/\delta \rceil = 1$, so at most one eigenvalue may be misclassified.  $\Phi$ is still within the graceful-degradation regime.
    \item For noise $\sigma = 0.5$: now $\sigma = \delta$, violating the condition $\sigma < \delta/4$.  The eigenvalue perturbation can create spurious near-zero modes, inflating $h^1$ and collapsing $\Phi$.
\end{itemize}

\textbf{Key takeaway:}
For well-separated spectra ($\delta \gg \sigma$), $\Phi$ is robust.  When $\sigma$ approaches $\delta$, a phase transition occurs where noise-induced eigenvalues cross the $\epsilon$ threshold, making $h^1$ estimation unreliable.  Running a controlled noise-injection study on our grid-world experiments is left to future work.

\textbf{Practical Guideline:} Ensure $\sigma < \delta/2$ through preprocessing (filtering, averaging, outlier removal).

%--------------------------------------------------

\subsection{Adaptive Threshold Selection}

\subsubsection{The Challenge of Choosing $\epsilon$}

The threshold $\epsilon$ separates "zero" eigenvalues (from $H^0$ and $H^1$) from "positive" eigenvalues (from higher modes). Choosing $\epsilon$ is critical:
\begin{itemize}
    \item \textbf{Too small:} Noise-induced perturbations may push true zero eigenvalues above $\epsilon$, undercounting $h^1$.
    \item \textbf{Too large:} Small positive eigenvalues may be mistaken for zeros, overcounting $h^1$.
\end{itemize}

\subsubsection{Strategy 1: Spectral Gap Estimation}

\textbf{Algorithm:}

\begin{enumerate}
    \item Compute the $k$ smallest eigenvalues: $\lambda_0, \lambda_1, \ldots, \lambda_{k-1}$ (using Lanczos, $k = 20$).
    \item Sort them: $\lambda_0 \leq \lambda_1 \leq \cdots \leq \lambda_{k-1}$.
    \item Identify the largest gap: $\delta_{\max} = \max_{i} (\lambda_{i+1} - \lambda_i)$.
    \item Set $\epsilon = \lambda_i + \delta_{\max}/2$ where $i$ is the index before the largest gap.
\end{enumerate}

\textbf{Intuition:} The spectral gap separates the "kernel block" (near-zero eigenvalues) from the "positive block". We place $\epsilon$ in the middle of this gap.

\textbf{Example:}

Eigenvalues: $[0.000, 0.002, 0.003, 0.500, 0.520, 0.540, \ldots]$

Gaps: $[0.002, 0.001, 0.497, 0.020, 0.020, \ldots]$

Largest gap: $0.497$ between $\lambda_2 = 0.003$ and $\lambda_3 = 0.500$.

Set $\epsilon = 0.003 + 0.497/2 \approx 0.25$.

Result: $h^1 = 3 - 1 = 2$ (three eigenvalues below $\epsilon$, minus one for $H^0$).

\subsubsection{Strategy 2: Noise-Adaptive Threshold}

\textbf{Algorithm:}

\begin{enumerate}
    \item Estimate noise level $\hat{\sigma}$ from data (e.g., via residual variance).
    \item Estimate spectral gap $\hat{\delta}$ (as above).
    \item Set $\epsilon = 2\hat{\sigma}$ (two standard deviations above zero).
    \item If $\epsilon > \hat{\delta}/2$, issue a warning: "Noise level too high for reliable $h^1$ estimation."
\end{enumerate}

\textbf{Rationale:} Eigenvalues perturbed by noise of magnitude $\sigma$ will fluctuate within $\pm \sigma$. Setting $\epsilon = 2\sigma$ ensures we don't miscount due to noise fluctuations (with 95\% confidence under Gaussian noise).

\subsubsection{Strategy 3: Cross-Validation}

\textbf{Algorithm:}

\begin{enumerate}
    \item Split data into $K$ folds (e.g., $K=5$).
    \item For each fold, construct the sheaf and compute $\Phi$ with various $\epsilon \in \{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}\}$.
    \item Select $\epsilon$ that minimizes variance of $\Phi$ across folds.
\end{enumerate}

\textbf{Rationale:} A good $\epsilon$ should give consistent $\Phi$ estimates across different data samples. High variance indicates $\epsilon$ is in an unstable region (near the spectral gap).

\subsubsection{Recommended Default}

For practitioners without domain-specific knowledge:

\begin{equation}
\epsilon = \max(10^{-3}, 2\hat{\sigma})
\end{equation}

This ensures:
\begin{itemize}
    \item Numerical stability (lower bound $10^{-3}$)
    \item Noise robustness (scales with estimated noise)
\end{itemize}

%--------------------------------------------------

\subsection{Noise Filtering Techniques}

\subsubsection{Preprocessing: Belief Smoothing}

\textbf{Idea:} Before constructing the sheaf, smooth the belief graph to reduce noise.

\textbf{Method 1: Spatial Averaging}

For graph-structured data, apply a diffusion filter:
\begin{equation}
\tilde{x}_v = \frac{1}{\deg(v) + 1} \left( x_v + \sum_{u \in \mathcal{N}(v)} x_u \right)
\end{equation}
where $\mathcal{N}(v)$ are the neighbors of $v$.

\textbf{Effect:} Reduces high-frequency noise while preserving large-scale structure.

\textbf{Method 2: Median Filtering}

For outlier-prone data:
\begin{equation}
\tilde{x}_v = \text{median}\{x_u : u \in \mathcal{N}(v) \cup \{v\}\}
\end{equation}

\textbf{Effect:} Robust to outliers (e.g., a single agent with a wildly incorrect belief).

\subsubsection{Postprocessing: Robust Spectral Estimation}

\textbf{Idea:} Use robust eigenvalue solvers that are less sensitive to noise.

\textbf{Method 1: Regularized Laplacian}

Add a small regularization term:
\begin{equation}
\mathcal{L}_{\text{reg}} = \mathcal{L} + \mu I
\end{equation}
where $\mu \approx 10^{-6}$.

\textbf{Effect:} Shifts all eigenvalues up by $\mu$, moving them away from the numerical zero threshold. This reduces false positives in $h^1$ counting.

\textbf{Method 2: Truncated Eigenvalue Decomposition}

Compute only the $k$ smallest eigenvalues (e.g., $k = 20$) using Lanczos. Ignore eigenvalues beyond $k$.

\textbf{Effect:} Focuses computation on the relevant part of the spectrum, avoiding numerical artifacts in the tail.

\subsubsection{Online Filtering: Kalman Smoothing}

For time-series data (e.g., RL training), apply a Kalman filter to $\Phi(t)$:
\begin{equation}
\hat{\Phi}(t) = \alpha \Phi(t) + (1 - \alpha) \hat{\Phi}(t-1)
\end{equation}
where $\alpha = 0.1$ is a smoothing parameter.

\textbf{Effect:} Reduces temporal noise, making $\Phi$ trends more interpretable.

%--------------------------------------------------

\subsection{Robustness Guarantees}

\subsubsection{Theorem: Bounded Degradation}

\textbf{Statement:} Under the assumptions of Theorem 2, if the noise level $\sigma < \delta/4$ and $\epsilon = \delta/2$, then:
\begin{equation}
\Phi(\mathcal{L}) \geq \frac{1}{2} \Phi(\mathcal{L}_0)
\end{equation}
with probability $\geq 1 - \exp(-N/2)$ (for Gaussian noise).

\textbf{Proof Sketch:}

\textit{Step 1:} By Weyl's theorem, $\lambda_1(\mathcal{L}) \geq \lambda_1(\mathcal{L}_0) - \sigma \geq \lambda_1(\mathcal{L}_0)/2$ (since $\sigma < \lambda_1/2$ by spectral gap assumption).

\textit{Step 2:} By Theorem 2, $h^1(\mathcal{L}) \leq h^1(\mathcal{L}_0) + 1$ (since $\lceil 2\sigma/\delta \rceil \leq 1$ when $\sigma < \delta/4$).

\textit{Step 3:} Combine:
\begin{equation}
\Phi(\mathcal{L}) = \frac{\lambda_1(\mathcal{L})}{h^1(\mathcal{L}) + \epsilon} \geq \frac{\lambda_1(\mathcal{L}_0)/2}{h^1(\mathcal{L}_0) + 1 + \epsilon} \geq \frac{1}{2} \cdot \frac{\lambda_1(\mathcal{L}_0)}{h^1(\mathcal{L}_0) + \epsilon} = \frac{1}{2} \Phi(\mathcal{L}_0)
\end{equation}
(assuming $h^1(\mathcal{L}_0) \geq 1$ for the inequality; if $h^1 = 0$, the bound is tighter). \qed

\textbf{Interpretation:} In low-noise regimes, $\Phi$ degrades by at most a factor of 2. This is a \textit{graceful degradation} property: the index remains useful even under perturbations.

\subsubsection{Failure Modes}

\textbf{When does the method break down?}

\begin{enumerate}
    \item \textbf{Noise $\gg$ Spectral Gap:} If $\sigma > \delta$, the spectrum is completely scrambled. $h^1$ estimation becomes unreliable.
    
    \textbf{Mitigation:} Apply aggressive filtering, increase graph connectivity (to increase $\delta$), or use alternative consistency measures (e.g., direct cohomology computation via linear algebra, accepting the $O(N^3)$ cost).
    
    \item \textbf{Adversarial Noise:} If noise is adversarially chosen to create spurious cycles, $h^1$ will be inflated.
    
    \textbf{Mitigation:} Use robust statistics (median, trimmed mean) instead of raw beliefs. Employ anomaly detection to identify and exclude adversarial agents.
    
    \item \textbf{Model Mismatch:} If the sheaf structure (restriction maps) does not accurately reflect true consistency relationships, $\Phi$ may give misleading signals.
    
    \textbf{Mitigation:} Validate the sheaf design on ground-truth data. Use domain expertise to refine restriction maps. Consider adaptive sheaf learning (future work).
\end{enumerate}

%--------------------------------------------------

\subsection{Experimental Validation of Robustness}

\subsubsection{Theoretical Noise Tolerance of the Grid-World Experiment}

The theoretical bounds derived above allow us to predict the robustness of $\Phi$ in our grid-world Bellman consistency experiment without running a separate noise-injection study. In the $8\times 8$ grid MDP:
\begin{itemize}
    \item The spectral gap of the Connection Laplacian is $\delta \approx 0.05$--$0.15$ (observed across 10 seeds).
    \item Applying the condition $\sigma < \delta/4$ gives a noise tolerance of roughly $\sigma < 0.01$--$0.04$.
    \item At typical Q-learning estimation noise ($\sigma \approx 0.01$ per stalk entry during late training), the eigenvalue perturbation bound predicts $|\Delta \lambda_1| \leq 0.01$, which is small relative to $\lambda_1 \approx 0.3$.
\end{itemize}

\textbf{Prediction:}
Based on the bounded-degradation theorem above, we expect $\Phi$ to remain within a factor of 2 of its noise-free value for our grid-world experiments, because the Q-learning estimation noise satisfies $\sigma < \delta/4$ once training has partially converged.  Validating this prediction under controlled noise injection (varying $\sigma$ explicitly) is left to future work.

\textbf{Practical implication:} Practitioners should verify that their measurement noise satisfies $\sigma < \delta/4$ for their specific graph structure before relying on $\Phi$ as a safety signal.  If this condition is not met, the noise-filtering techniques described below should be applied first.

\subsubsection{Expected Benefit of Adaptive $\epsilon$ Selection}

\textbf{Intuition:}
The three strategies differ in how they handle noise:
\begin{itemize}
    \item \textbf{Fixed $\epsilon = 10^{-3}$:} Works well when noise is negligible ($\sigma \ll 10^{-3}$) but misclassifies eigenvalues when noise grows.
    \item \textbf{Gap-based $\epsilon = \delta_{\max}/2$:} Adapts to spectrum structure; robust when the spectral gap is clearly identifiable.
    \item \textbf{Adaptive $\epsilon = 2\hat{\sigma}$:} Tracks the noise level directly; should degrade gracefully as noise increases.
\end{itemize}

The theoretical analysis predicts that adaptive methods will dominate fixed thresholds at moderate-to-high noise, because fixed $\epsilon$ either under- or over-counts eigenvalues as the noise floor shifts.  We use the gap-based strategy in all our experiments and recommend it as the default for practitioners.  A rigorous empirical comparison of these three strategies across multiple noise regimes is left to future work.

\textbf{Recommendation:} We recommend using the gap-based or adaptive strategy in practice.

%--------------------------------------------------

\subsection{Practical Recommendations}

\textbf{For Practitioners:}

\begin{enumerate}
    \item \textbf{Estimate Noise Level:} Before deployment, characterize the noise in your system (e.g., via test data or simulation).
    
    \item \textbf{Choose $\epsilon$ Adaptively:} Use the spectral gap method (Section~\ref{sec:parameter_selection}) rather than a fixed threshold.
    
    \item \textbf{Preprocess Data:} Apply spatial averaging or median filtering to reduce noise before constructing the sheaf.
    
    \item \textbf{Monitor $\Phi$ Trends:} Use temporal smoothing (Kalman filter) to track $\Phi(t)$ over time, focusing on trends rather than instantaneous values.
    
    \item \textbf{Set Alerts:} Define a threshold $\Phi_{\text{crit}}$ (e.g., $\Phi < 0.5$) below which the system should trigger a warning or initiate a recovery protocol.
    
    \item \textbf{Validate on Ground Truth:} If possible, compute $h^1$ directly on a small subset of data to validate the spectral approximation.
\end{enumerate}

\textbf{For Researchers:}

\begin{enumerate}
    \item \textbf{Extend to Non-Gaussian Noise:} Current analysis assumes Gaussian noise. Investigate robustness under heavy-tailed or adversarial noise distributions.
    
    \item \textbf{Adaptive Sheaf Learning:} Develop methods to learn restriction maps from data, reducing reliance on manual design.
    
    \item \textbf{Higher-Order Robustness:} Analyze sensitivity of higher cohomology groups ($H^2, H^3, \ldots$) for simplicial complex extensions.
    
    \item \textbf{Real-Time Adaptation:} Design online algorithms that adjust $\epsilon$ and filtering parameters dynamically as the system evolves.
\end{enumerate}
